{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = { \n",
    "            'data_size': None, # size of data subset after shuffle is performed\n",
    "            'test_size': 0.25, # fraction of data set to be assigned as test data\n",
    "            'save_env': False, # save environment\n",
    "            'del': True, # Delete variables that are no longer needed to proceed in computations to save place\n",
    "            'learning_curve': { # parameters to generate learning_curve\n",
    "                'start': int(5e4), # training set start size (including)\n",
    "                'stop': int(1e6 + 2.5e4), # traing set stop size (excluding)\n",
    "                'step': int(2.5e4)  # increase between iterations\n",
    "#                 'start': int(5e2), # training set start size (including)\n",
    "#                 'stop': int(1e4 + 2.5e2), # traing set stop size (excluding)\n",
    "#                 'step': int(2.5e2)  # increase between iterations\n",
    "                              }\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data: load dataset, shuffle it and take subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = pd.read_csv(f'{PATH}train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "data_shuffled = shuffle(data_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (settings['del']):\n",
    "    del data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_shuffled.iloc[:settings['data_size'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (settings['del']):\n",
    "    del data_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split category_name into main_cat, subcat_1 and subcat_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: BuryBuryZymon at https://www.kaggle.com/maheshdadhich/i-will-sell-everything-for-free-0-55\n",
    "def split_cat(text):\n",
    "    try: return text.split(\"/\")\n",
    "    except: return (\"No Label\", \"No Label\", \"No Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,'main_cat'], data.loc[:,'subcat_1'], data.loc[:,'subcat_2'] = \\\n",
    "zip(*data.loc[:,'category_name'].apply(lambda x: split_cat(x)))\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique fields:\\n\")\n",
    "\n",
    "print(\"main_cat: \\t%d\" % data['main_cat'].nunique())\n",
    "print(\"subcat_1: \\t%d\" % data['subcat_1'].nunique())\n",
    "print(\"subcat_2: \\t%d\" % data['subcat_2'].nunique())\n",
    "print(\"brand_name: \\t%d\" % data['brand_name'].nunique())\n",
    "print()\n",
    "\n",
    "print(\"%d items have no category\" % len(data.loc[data['main_cat'] == 'No Label']))\n",
    "print(\"%d items have no brand\" % data['brand_name'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerically represent features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_id: copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new DataFrame called data_num for numerical representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = pd.DataFrame(data.loc[:,'train_id'], columns=['train_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### name: represent name by its length as name_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num.loc[:,'name_len'] = data['name'].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### item_condition_id, price, shipping: copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num[['item_condition_id', 'price', 'shipping']] = data.loc[:,['item_condition_id', 'price', 'shipping']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### item_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represent item_description by its length as item_description_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_description_len = data.loc[:,'item_description'].str.len()\n",
    "data_num['item_description_len'] = item_description_len\n",
    "\n",
    "# Replace NaN in data_num.item_description_len column by zeros\n",
    "data_num['item_description_len'] = data_num['item_description_len'].fillna(0)\n",
    "\n",
    "# Change data type of this column to uint16 provided the max val is less than 65535\n",
    "if (data_num.item_description_len.max() < 65535):\n",
    "    data_num['item_description_len'] = data_num['item_description_len'].astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function for making binary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binary_columns(df_str, df_num, column_name):\n",
    "    \"\"\"\n",
    "    Turns a single column named column_name (with various categories) into m binary columns, where m is the number\n",
    "    of unique values in the original column. For each sample, the value for all new columns is 0 apart the one\n",
    "    that matches the value of the original column. Names of new binary columns are formed as follows:\n",
    "    column_name + '_' + str(original column value)\n",
    "    Inputs:\n",
    "        two pandas DataFrames: df_str where a single column contains information about given category\n",
    "                               df_num that will later contain many binary columns\n",
    "        column_name: name of the column that will be split into several binary columns\n",
    "    Returns nothing. It appends the collumns directly into df_num to increase performance.\n",
    "    \"\"\"\n",
    "    m = len(df_str)\n",
    "    if (m != len(df_num)):\n",
    "        raise Exception(\"df_str and df_num must have the same size.\")\n",
    " \n",
    "    categories = df_str[column_name].unique()\n",
    "\n",
    "    from tqdm import tqdm_notebook # progress bar\n",
    "\n",
    "    # Create a new feature for each category and initialize it to 0\n",
    "    for i in tqdm_notebook(categories, desc='1/2'):\n",
    "        df_num[column_name + '_' + str(i)] = np.zeros((m, 1), dtype=np.int8)\n",
    "\n",
    "#     Loop thorugh all rows and assign 1 to the column whose name is the same as category\n",
    "    for i in tqdm_notebook(df_str.index, desc='2/2'): # loop through all rows\n",
    "        category = str(df_str.at[i, column_name])\n",
    "        df_num.at[i, column_name + '_' + category] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### brand_name: for each unique one create new binary feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_binary_columns(data, data_num, 'brand_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main_cat, subcat_1, subcat_2: for each unique one create new binary feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_binary_columns(data, data_num, 'main_cat')\n",
    "make_binary_columns(data, data_num, 'subcat_1')\n",
    "make_binary_columns(data, data_num, 'subcat_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (settings['del']):\n",
    "    del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data and extract X, y and train_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_unscaled, X_test_unscaled = train_test_split(data_num, test_size = settings['test_size'], random_state=None) # randomly split data\n",
    "# ! X_train_unscaled and X_test_unscaled STILL CONTAINS PRICE AT THIS MOMENT !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (settings['del']):\n",
    "    del data_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pop price from X_train and X_test\n",
    "y_train = X_train_unscaled.pop('price')\n",
    "y_test = X_test_unscaled.pop('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pop id_train from both training and test data set\n",
    "\n",
    "id_train = X_train_unscaled.pop('train_id')\n",
    "id_test = X_test_unscaled.pop('train_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "X_train = X_train_unscaled.copy()\n",
    "X_test = X_test_unscaled.copy()\n",
    "\n",
    "columns_to_scale = ['name_len', 'item_condition_id', 'item_description_len']\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "scaler.fit(X_train_unscaled[columns_to_scale]) # Compute the mean adn std of training data to be used for future scaling\n",
    "\n",
    "X_train[columns_to_scale] = pd.DataFrame(scaler.transform(X_train_unscaled[columns_to_scale]), index=X_train_unscaled.index, columns=columns_to_scale)\n",
    "if (settings['del']):\n",
    "    del X_train_unscaled\n",
    "\n",
    "X_test[columns_to_scale] = pd.DataFrame(scaler.transform(X_test_unscaled[columns_to_scale]), index=X_test_unscaled.index, columns=columns_to_scale)\n",
    "if (settings['del']):\n",
    "    del X_test_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_unscaled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_unscaled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.to_csv('X_train.csv')\n",
    "# X_train_scaled.to_csv('X_train_scaled.csv')\n",
    "# X_test.to_csv('X_test.csv')\n",
    "# X_test_scaled.to_csv('X_test_scaled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (settings['save_env']):\n",
    "    import dill                            #pip install dill --user\n",
    "    dill.dump_session('splittedData.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check size of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' By Fred Cirera, after https://stackoverflow.com/a/1094933/1870254'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name,value in locals().items()),\n",
    "                         key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name,sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance measure: RMSE\n",
    "\n",
    "$$\\text{RMSE} \\left( \\mathbf{Y} , \\mathbf{\\hat{Y}} \\right) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\hat{y_i} \\right)^2 } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_test, y_pred):\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    return np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning curve for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook # progress bar\n",
    "\n",
    "# Initialize arrays for plotting learning curves\n",
    "training_set_sizes = []\n",
    "train_rmses = []\n",
    "test_rmses = []\n",
    "\n",
    "# Create file with headings to save training and test RMSEs for learning curves\n",
    "import datetime\n",
    "filename = datetime.datetime.now().strftime('%y%m%d') + '_' + 'learning_curve.csv' # create file name starting with yymmdd_\n",
    "with open('generated_data/' + filename, 'w') as f:\n",
    "    f.write('training_set_size,training_error,test_error\\n')\n",
    "\n",
    "# Generate learning curves and save them\n",
    "for m in tqdm_notebook(range(settings['learning_curve']['start'], settings['learning_curve']['stop'], settings['learning_curve']['step'])):\n",
    "    # Slice dataset\n",
    "    X_train_red = X_train[:m]\n",
    "    y_train_red = y_train[:m]\n",
    "    \n",
    "    # Applu linear regression\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    lnr_regr = LinearRegression(n_jobs=-1)\n",
    "    print(\"Trainig for training set size of: \", m, \"...\")\n",
    "    lnr_regr.fit(X_train_red, y_train_red)\n",
    "    \n",
    "    # Make predictions and compute RMSEs\n",
    "    pred_train = lnr_regr.predict(X_train_red)\n",
    "    rmse_train = rmse(y_train_red, pred_train)\n",
    "    print(\"Training set RMSE: %.2f\" % rmse_train)\n",
    "    pred_test = lnr_regr.predict(X_test)\n",
    "    rmse_test = rmse(y_test, pred_test)\n",
    "    print(\"Training set RMSE: %.2f\" % rmse_test)\n",
    "    \n",
    "    # Save to csv file\n",
    "    print(\"Saving to file...\")\n",
    "    with open('generated_data/' + filename, 'a') as f:\n",
    "        f.write(str(m) + ',' + str(rmse_train) + ',' + str(rmse_test) +'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply linear regression to the full trainin set, compute training and test RMSE and add them to the file created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lnr_regr = LinearRegression(n_jobs=-1)\n",
    "print(\"Trainig...\")\n",
    "lnr_regr.fit(X_train, y_train)\n",
    "print(\"Training done.\")\n",
    "\n",
    "if (settings['save_env']):\n",
    "    dill.dump_session('linearModel.pkl')\n",
    "    \n",
    "# Make predictions and report train and test RMSEs\n",
    "\n",
    "print(\"Evaluating performance on the training set...\")\n",
    "pred_train = lnr_regr.predict(X_train)\n",
    "rmse_train = rmse(y_train, pred_train)\n",
    "print(\"Training set RMSE: %.2f\" % rmse_train)\n",
    "\n",
    "print(\"Evaluating performance on the test test...\")\n",
    "pred_test = lnr_regr.predict(X_test)\n",
    "rmse_test = rmse(y_test, pred_test)\n",
    "print(\"Test set RMSE: %.2f\" % rmse_test)\n",
    "\n",
    "# Save to csv file\n",
    "print(\"Saving to file...\")\n",
    "with open('generated_data/' + filename, 'a') as f:\n",
    "    f.write(str(len(X_train)) + ',' + str(rmse_train) + ',' + str(rmse_test) +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dill.load_session('linearModel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
